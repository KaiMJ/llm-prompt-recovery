{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkim/miniconda3/envs/kaggle/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import TextStreamer\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\"\n",
    "prompt = \"Convert this into a sea shanty: \"\n",
    "rewritten_text = \"Here is your shanty: \" + \\\n",
    "    \"(Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. \" + \\\n",
    "    \"(Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.\" + \\\n",
    "    \"(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/gemma-7b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
    "streamer = TextStreamer(tokenizer=tokenizer, skip_prompt=True)\n",
    "sentenceTF = SentenceTransformer('sentence-transformers/sentence-t5-base')\n",
    "streamer = TextStreamer(tokenizer=tokenizer, skip_prompt=True)\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer(text)[\"input_ids\"])\n",
    "\n",
    "def sim(s, k, p=3):\n",
    "    sign = np.sign(np.dot(s, k))\n",
    "    s_norm = np.linalg.norm(s)\n",
    "    k_norm = np.linalg.norm(k)\n",
    "\n",
    "    abs_dot_product = np.abs(np.dot(s, k)) ** p\n",
    "    result = sign * (abs_dot_product / (s_norm * k_norm))\n",
    "    return result\n",
    "\n",
    "def get_sim(text1, text2):\n",
    "    embeddings = sentenceTF.encode([text1, text2])\n",
    "    return sim(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = prompt + original_text\n",
    "# input_text = f\"What is the prompt that changed the original text to the rewriten text? \\n Here is the original text ``` {original_text} ``` \\nHere is the rewritten_text ```{rewritten_text}```\"\n",
    "# input_text = f\"You are an expert prompt reverse engineer. Give me as details as you can about what the text seems like: ```{rewritten_text}```\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 224 seems to be generated max tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**Shanty:**\n",
      "\n",
      "(Verse 1) The text has been rewritten, a tale spun anew,\n",
      "By the Gemma LLM, with a rewrite prompt too.\n",
      "The goal is to find, the prompt that was used,\n",
      "To unlock the secrets, hidden in the text.\n",
      "\n",
      "(Chorus) Oh, me hearties, let's sing along,\n",
      "The competition's on, where we belong.\n",
      "With code and cunning, we'll crack the code,\n",
      "And find the prompt, with all our might.\n",
      "\n",
      "(Verse 2) The text passages, they dance and flow,\n",
      "But the prompt remains\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**input_ids, max_length=224, streamer=streamer)\n",
    "# print(tokenizer.decode(outputs[0]))\n",
    "# count_tokens(tokenizer.decode(outputs[0]).replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n**Shanty:**\\n\\n(Verse 1) The text has been rewritten, a tale spun anew,\\nBy the Gemma LLM, with a rewrite prompt too.\\nThe goal is to find, the prompt that was used,\\nTo unlock the secrets, hidden in the text.\\n\\n(Chorus) Oh, me hearties, let's sing along,\\nThe competition's on, where we belong.\\nWith code and cunning, we'll crack the code,\\nAnd find the prompt, with all our might.\\n\\n(Verse 2) The text passages, they dance and flow,\\nBut the prompt remains\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated = tokenizer.decode(outputs[:, input_ids[\"input_ids\"].shape[1]:][0])\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8815261577554485"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sim(generated, rewritten_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"public_10k_unique_rewrite_prompt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rewrite_prompt.to_csv(\"rewrite.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'phi2_public_data_sft'\n",
    "data_path = 'public_10k_unique_rewrite_prompt.csv'\n",
    "model_path = \"microsoft/phi-2\"\n",
    "output_path = f'outputs'\n",
    "model_save_path =  f'{exp_name}_adapter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5\n",
    "batch_size=1 # 2 \n",
    "max_seq_length=512 # 1024 \n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "train_df, val_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/mkim/miniconda3/envs/kaggle/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Downloading shards: 100%|██████████| 2/2 [03:12<00:00, 96.50s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype='float16',\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             trust_remote_code=True,\n",
    "                                             use_auth_token=True)\n",
    "model.config.gradient_checkpointing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_len(text):\n",
    "    tokenized = tokenizer(text, return_length=True)\n",
    "    length = tokenized['length'][0]\n",
    "    return length\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['rewritten_text'])):\n",
    "        ori_text = example['original_text'][i]\n",
    "        rew_text = example['rewritten_text'][i]\n",
    "        rew_prompt = example['rewrite_prompt'][i]\n",
    "        text = f\"Instruct: Original Text:{ori_text}\\nRewritten Text:{rew_text}\\nWrite a prompt that was likely given to the LLM to rewrite original text into rewritten text.Output: {rew_prompt}\"\n",
    "        if token_len(text) > max_seq_length:\n",
    "            continue\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"Output:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template=response_template, \n",
    "                                           tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir = output_path,\n",
    "    fp16=True,\n",
    "    learning_rate=lr,\n",
    "    optim=\"adafactor\",\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size*2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_steps=50,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    report_to='none',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   0%|          | 0/7399 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2159 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 7399/7399 [00:05<00:00, 1247.09 examples/s]\n",
      "Map: 100%|██████████| 3172/3172 [00:02<00:00, 1087.06 examples/s]\n",
      "/home/mkim/miniconda3/envs/kaggle/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args = args,\n",
    "    max_seq_length=max_seq_length,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkim/miniconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1320' max='1320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1320/1320 2:11:20, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.228200</td>\n",
       "      <td>1.134236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.035700</td>\n",
       "      <td>1.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.886600</td>\n",
       "      <td>0.953702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.937026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.843800</td>\n",
       "      <td>0.934122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkim/miniconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/mkim/miniconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/mkim/miniconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/mkim/miniconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1320, training_loss=1.059759164578987, metrics={'train_runtime': 7885.3166, 'train_samples_per_second': 2.679, 'train_steps_per_second': 0.167, 'total_flos': 1.309273021254144e+17, 'train_loss': 1.059759164578987, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('phi2/tokenizer_config.json',\n",
       " 'phi2/special_tokens_map.json',\n",
       " 'phi2/vocab.json',\n",
       " 'phi2/merges.txt',\n",
       " 'phi2/added_tokens.json',\n",
       " 'phi2/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = \"phi2\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
